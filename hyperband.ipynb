{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ebe2052-dec8-40e0-bf40-fd80d23ab461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datasets\n",
    "import pandas\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import (L1,L2) \n",
    "from tensorflow.keras.layers import (SimpleRNN, Dense, Conv1D, Conv2D, MaxPooling2D,\n",
    "                                      Flatten, Bidirectional, LSTM, GRU, Embedding, \n",
    "                                      Dropout, GlobalMaxPooling1D, GlobalAveragePooling1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87577293-628f-48ee-891c-ba19a545dc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.13.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\hewyu\\anaconda3\\lib\\site-packages\n",
      "Requires: tensorflow-intel\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ee6ecc-17ef-4dec-8108-024b29763ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfad428d2a0f412da7cf480a5230973c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16add222c034484b238118398b37f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf4eccc8413478aade1251207f0cc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hewyu\\anaconda3\\lib\\site-packages\\pyarrow\\pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f603790847ee485a96636650b04feb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hewyu\\anaconda3\\lib\\site-packages\\pyarrow\\pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4874dd29af43a6b7f78be4ae6499a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2410b5f39e0488796712e8d62f8c263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e28ce609bdb4a3d87fdbabb5f8f1385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2520 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a449bdd3be2a497a997288010ca5f7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the tokenizer from DistilRoBERTa\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilroberta-base\")\n",
    "\n",
    "def tokenize(examples):\n",
    "    \"\"\"Converts the text of each example to \"input_ids\", a sequence of integers\n",
    "    representing 1-hot vectors for each token in the text\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=64,\n",
    "                     padding=\"max_length\")\n",
    "\n",
    "model_path=\"model\"\n",
    "train_path=\"../graduate-project-data/train_10s.csv\"\n",
    "dev_path=\"../graduate-project-data/dev_10s.csv\"\n",
    "\n",
    "# load the CSVs into Huggingface datasets to allow use of the tokenizer\n",
    "hf_dataset = datasets.load_dataset(\"csv\", data_files={\n",
    "    \"train\": train_path, \"validation\": dev_path})\n",
    "\n",
    "# the labels are the names of all columns except the first\n",
    "labels = hf_dataset[\"train\"].column_names[1:]\n",
    "\n",
    "def gather_labels(example):\n",
    "    \"\"\"Converts the label columns into a list of 0s and 1s\"\"\"\n",
    "    # the float here is because F1Score requires floats\n",
    "    return {\"labels\": [float(example[l]) for l in labels]}\n",
    "\n",
    "# convert text and labels to format expected by model\n",
    "hf_dataset = hf_dataset.map(gather_labels)\n",
    "hf_dataset = hf_dataset.map(tokenize, batched=True)\n",
    "#hf_dataset = hf_dataset.map(to_bow) # For Feed Forward NN\n",
    "\n",
    "# convert Huggingface datasets to Tensorflow datasets\n",
    "train_dataset = hf_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=\"input_ids\", # \"input_bow\" for FF\n",
    "    label_cols=\"labels\", \n",
    "    batch_size=16,\n",
    "    shuffle=True)\n",
    "dev_dataset = hf_dataset[\"validation\"].to_tf_dataset(\n",
    "    columns=\"input_ids\", # input_bow for FF\n",
    "    label_cols=\"labels\",\n",
    "    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8547fc3b-3332-4e18-98c7-d865a8d9b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/76109047/integrate-batch-size-in-keras-tuner\n",
    "# https://keras.io/guides/keras_tuner/getting_started/#tune-model-training\n",
    "class my_model:\n",
    "\n",
    "    def model_builder(self, hp):\n",
    "    \n",
    "        # Bidiretional GRU model\n",
    "        model = Sequential()\n",
    "    \n",
    "        hp_output_dim = hp.Choice('output_dim', values=[32, 64, 128, 256])\n",
    "        hp_gru = hp.Choice('gru_units', values=[32, 64, 128, 256])\n",
    "        hp_do1 = hp.Float('dropout_1', min_value=0.1, max_value=1, step=0.1)\n",
    "        hp_d =  hp.Choice('dense', values=[32, 64, 128, 256])\n",
    "        hp_do2 = hp.Float('dropout_2', min_value=0.1, max_value=1, step=0.1)\n",
    "        #hp_layer = hp.Choice('layer_type', values=[Flatten(), GlobalMaxPooling1D(), GlobalAveragePooling1D()])\n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "        hp_thresh = hp.Float('threshold', min_value=0.1, max_value=1, step=0.1)\n",
    "        self.hp_batch_size = hp.Choice('batch_size', values=[16, 32, 64])\n",
    "    \n",
    "        model.add(Embedding(input_dim=tokenizer.vocab_size, output_dim=hp_output_dim, input_length=64))\n",
    "        model.add(Bidirectional(GRU(units=hp_gru, return_sequences=True), input_shape=(tokenizer.vocab_size,)))\n",
    "        model.add(Dropout(rate=hp_do1)) \n",
    "        model.add(Dense(units=hp_d, activation='relu')) #model.add(Dense(64, activation='relu', kernel_regularizer=L2(0.01)))\n",
    "        model.add(Dropout(rate=hp_do2))\n",
    "        model.add(Flatten()) \n",
    "        model.add(Dense(len(labels), activation='sigmoid'))\n",
    "    \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "            loss=tf.keras.losses.binary_crossentropy,\n",
    "            metrics=[tf.keras.metrics.F1Score(average=\"micro\", threshold=hp_thresh)])\n",
    "      \n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=self.hp_batch_size,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bda303e-d08e-4f8e-8313-9ec47610cc2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Specify the objective with direction\n",
    "objective = kt.Objective(\"val_f1_score\", direction=\"max\")\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective=objective,\n",
    "                     max_epochs=20,\n",
    "                     factor=3,\n",
    "                     overwrite=True,\n",
    "                     directory='.',\n",
    "                     project_name='hp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03881060-a459-4a94-9f1c-b0b43d274e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 05m 11s]\n",
      "val_f1_score: 0.5849056839942932\n",
      "\n",
      "Best val_f1_score So Far: 0.7049808502197266\n",
      "Total elapsed time: 02h 54m 28s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train_dataset, \n",
    "                epochs=20, \n",
    "                #batch_size=hp_batch_size,\n",
    "                validation_data=dev_dataset,\n",
    "                callbacks=[tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=model_path,\n",
    "                monitor=\"val_f1_score\",\n",
    "                mode=\"max\",\n",
    "                save_best_only=True),\n",
    "                EarlyStopping(monitor='val_f1_score', mode='max', patience=3, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba9f720b-b11f-43dd-8903-6a34de827230",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'trials'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m oracle_state \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[1;32m----> 2\u001b[0m trials \u001b[38;5;241m=\u001b[39m \u001b[43moracle_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrials\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of trials executed:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trials))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'trials'"
     ]
    }
   ],
   "source": [
    "oracle_state = tuner.oracle.get_state()\n",
    "trials = oracle_state['trials']\n",
    "print(\"Number of trials executed:\", len(trials))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea6b1a53-39f7-4b15-86b9-208cdc1a181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "output_dim: 128\n",
      "gru_units: 128\n",
      "dropout_1: 0.1\n",
      "dense: 64\n",
      "dropout_2: 0.1\n",
      "learning_rate: 0.001\n",
      "threshold: 0.4\n",
      "batch_size: 64\n",
      "tuner/epochs: 3\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n"
     ]
    }
   ],
   "source": [
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in best_hyperparameters.values.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96d8ab47-4baa-4d93-9612-475cdbcee2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 64), dtype=tf.int64, name=None), TensorSpec(shape=(None, 7), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d3256-2a2b-4adc-a552-f81a7fefecd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
